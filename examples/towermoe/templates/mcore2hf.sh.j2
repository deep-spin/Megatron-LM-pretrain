#!/bin/bash

set -euo pipefail

{{ launch.activate_env_cmd }}

MEGATRON_CKPTS_DIR={{ checkpointing.megatron_ckpts_dir }}
HF_CKPTS_DIR={{ checkpointing.hf_ckpts_dir }}
HF_TOKENIZER_PATH={{ checkpointing.hf_tokenizer_path }}
RUN_DIR={{ launch.run_dir }}

TP_SIZE={{ distributed.tensor_model_parallel_size }}
PP_SIZE={{ distributed.pipeline_model_parallel_size }}
EP_SIZE={{ distributed.expert_model_parallel_size }}

cd {{ megatron_dir }}

# Add Megatron-LM to the PYTHONPATH
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
MEGATRON_DIR="$( cd "$( dirname "$( dirname "${SCRIPT_DIR}" )" )" && pwd )"

if [ -z "${PYTHONPATH+x}" ]; then
    export PYTHONPATH="${MEGATRON_DIR}"
else
    export PYTHONPATH="${MEGATRON_DIR}:${PYTHONPATH}"
fi

mkdir -p "$HF_CKPTS_DIR"

# Loop over all iter_{iter num} subdirectories in the base directory
for LOAD_DIR in "$MEGATRON_CKPTS_DIR"/iter_*; do
    if [ -d "$LOAD_DIR" ]; then
        # Extract the iteration number
        ITER_NUM=$(basename "$LOAD_DIR" | sed 's/^iter_//')

        # Define the save directory for the converted model
        SAVE_DIR="$HF_CKPTS_DIR/$ITER_NUM"

        # Skip if the save directory already exists
        if [ -d "$SAVE_DIR" ]; then
            echo "Skipping iteration $ITER_NUM: $SAVE_DIR already exists"
            continue
        fi

        JOB_NAME="convert-$ITER_NUM"
        OUTPUT_FILE="$RUN_DIR/$JOB_NAME.out"
        ERROR_FILE="$RUN_DIR/$JOB_NAME.err"

        SRUN_ARGS="\
            --ntasks 1
            --ntasks-per-node 1 \
            --cpus-per-task 8 \
            --gpus-per-task 1 \
            --hint=nomultithread \
            --output=$OUTPUT_FILE \
            --error=$ERROR_FILE \
            --job-name=$JOB_NAME \
            --time=00:30:00 \
            {%- if 'account' in launch and launch.account %}
            --account={{ launch.account }} \
            {%- endif -%}
            {%- if 'partition' in launch and launch.partition %}
            --partition={{ launch.partition }} \
            {%- endif -%}
            {%- if 'qos' in launch and launch.qos %}
            --qos={{ launch.qos }} \
            {%- endif %}
            "

        # Call the convert.py script with the required arguments
        srun $SRUN_ARGS -- \
            python examples/mixtral/convert.py mcore-to-hf \
            --mcore-load-dir "$LOAD_DIR" \
            --hf-save-dir "$SAVE_DIR" \
            --hf-tokenizer-path "$HF_TOKENIZER_PATH" \
            --source-tensor-parallel-size "$TP_SIZE" \
            --source-pipeline-parallel-size "$PP_SIZE" \
            --source-expert-parallel-size "$EP_SIZE" \
            --target-params-dtype bfloat16 \
            --moe-grouped-gemm &
    fi
done

wait
